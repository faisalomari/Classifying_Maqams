num_epoch = 50
batch = 64

results:
Epoch 01: train_loss=4.67432, val_loss=19.26527, val_acc=0.11392
Epoch 01: train_loss=20.07736, val_loss=16.54338, val_acc=0.17722
Epoch 01: train_loss=16.84320, val_loss=19.38837, val_acc=0.27848
Epoch 01: train_loss=15.34023, val_loss=25.44695, val_acc=0.37975
Epoch 01: train_loss=27.18782, val_loss=29.10080, val_acc=0.25316
Epoch 02: train_loss=33.23089, val_loss=27.28960, val_acc=0.26582
Epoch 02: train_loss=32.82777, val_loss=22.67518, val_acc=0.27848
Epoch 02: train_loss=22.59962, val_loss=20.95652, val_acc=0.27848
Epoch 02: train_loss=20.77201, val_loss=19.58653, val_acc=0.31646
Epoch 02: train_loss=14.30399, val_loss=18.15844, val_acc=0.22785
Epoch 03: train_loss=16.02037, val_loss=16.83441, val_acc=0.22785
Epoch 03: train_loss=16.41281, val_loss=10.69116, val_acc=0.32911
Epoch 03: train_loss=8.98397, val_loss=7.71251, val_acc=0.37975
Epoch 03: train_loss=6.91119, val_loss=6.81625, val_acc=0.37975
Epoch 03: train_loss=3.10723, val_loss=7.70120, val_acc=0.39241
Epoch 04: train_loss=6.02206, val_loss=9.14674, val_acc=0.32911
Epoch 04: train_loss=6.76871, val_loss=9.98149, val_acc=0.29114
Epoch 04: train_loss=8.19445, val_loss=10.44766, val_acc=0.30380
Epoch 04: train_loss=5.41261, val_loss=9.23157, val_acc=0.31646
Epoch 04: train_loss=5.98765, val_loss=7.10886, val_acc=0.39241
Epoch 05: train_loss=4.18852, val_loss=5.13714, val_acc=0.51899
Epoch 05: train_loss=2.44162, val_loss=3.56972, val_acc=0.59494
Epoch 05: train_loss=1.72710, val_loss=3.08757, val_acc=0.49367
Epoch 05: train_loss=1.26520, val_loss=3.91821, val_acc=0.40506
Epoch 05: train_loss=2.56969, val_loss=4.50096, val_acc=0.44304
Epoch 06: train_loss=2.42885, val_loss=4.16438, val_acc=0.48101
Epoch 06: train_loss=1.91875, val_loss=3.96165, val_acc=0.50633
Epoch 06: train_loss=2.48848, val_loss=3.82187, val_acc=0.50633
Epoch 06: train_loss=1.96979, val_loss=3.58355, val_acc=0.53165
Epoch 06: train_loss=1.31730, val_loss=3.39626, val_acc=0.55696
Epoch 07: train_loss=0.86284, val_loss=3.50543, val_acc=0.51899
Epoch 07: train_loss=2.95041, val_loss=3.37103, val_acc=0.51899
Epoch 07: train_loss=1.63757, val_loss=2.95036, val_acc=0.51899
Epoch 07: train_loss=0.92640, val_loss=2.53808, val_acc=0.48101
Epoch 07: train_loss=0.17093, val_loss=2.39672, val_acc=0.53165
Epoch 08: train_loss=0.40722, val_loss=2.43136, val_acc=0.50633
Epoch 08: train_loss=0.41515, val_loss=2.51255, val_acc=0.53165
Epoch 08: train_loss=0.20740, val_loss=2.54932, val_acc=0.54430
Epoch 08: train_loss=0.48949, val_loss=2.51935, val_acc=0.54430
Epoch 08: train_loss=0.41908, val_loss=2.46028, val_acc=0.55696
Epoch 09: train_loss=0.21554, val_loss=2.44346, val_acc=0.55696
Epoch 09: train_loss=0.32964, val_loss=2.43387, val_acc=0.55696
Epoch 09: train_loss=0.19979, val_loss=2.41505, val_acc=0.55696
Epoch 09: train_loss=0.33473, val_loss=2.36801, val_acc=0.55696
Epoch 09: train_loss=0.29062, val_loss=2.22695, val_acc=0.56962
Epoch 10: train_loss=0.08542, val_loss=2.12141, val_acc=0.56962
Epoch 10: train_loss=0.15625, val_loss=2.03066, val_acc=0.65823
Epoch 10: train_loss=0.17262, val_loss=1.95830, val_acc=0.67089
Epoch 10: train_loss=0.10390, val_loss=1.90748, val_acc=0.65823
Epoch 10: train_loss=0.14748, val_loss=1.87060, val_acc=0.64557
Epoch 11: train_loss=0.02427, val_loss=1.85888, val_acc=0.60759
Epoch 11: train_loss=0.03065, val_loss=1.86355, val_acc=0.58228
Epoch 11: train_loss=0.07204, val_loss=1.87525, val_acc=0.56962
Epoch 11: train_loss=0.06660, val_loss=1.86458, val_acc=0.56962
Epoch 11: train_loss=0.11410, val_loss=1.83673, val_acc=0.55696
Epoch 12: train_loss=0.05111, val_loss=1.79287, val_acc=0.56962
Epoch 12: train_loss=0.03763, val_loss=1.76006, val_acc=0.58228
Epoch 12: train_loss=0.01840, val_loss=1.73618, val_acc=0.59494
Epoch 12: train_loss=0.04124, val_loss=1.71469, val_acc=0.58228
Epoch 12: train_loss=0.04432, val_loss=1.69578, val_acc=0.58228
Epoch 13: train_loss=0.01824, val_loss=1.68526, val_acc=0.58228
Epoch 13: train_loss=0.02621, val_loss=1.67918, val_acc=0.56962
Epoch 13: train_loss=0.01275, val_loss=1.67760, val_acc=0.56962
Epoch 13: train_loss=0.02439, val_loss=1.67133, val_acc=0.56962
Epoch 13: train_loss=0.02222, val_loss=1.65926, val_acc=0.56962
Epoch 14: train_loss=0.00951, val_loss=1.65062, val_acc=0.58228
Epoch 14: train_loss=0.01960, val_loss=1.64123, val_acc=0.59494
Epoch 14: train_loss=0.01334, val_loss=1.63252, val_acc=0.59494
Epoch 14: train_loss=0.01625, val_loss=1.62355, val_acc=0.58228
Epoch 14: train_loss=0.00804, val_loss=1.61737, val_acc=0.59494
Epoch 15: train_loss=0.00933, val_loss=1.61182, val_acc=0.59494
Epoch 15: train_loss=0.00960, val_loss=1.60651, val_acc=0.59494
Epoch 15: train_loss=0.01538, val_loss=1.60119, val_acc=0.59494
Epoch 15: train_loss=0.00838, val_loss=1.59753, val_acc=0.59494
Epoch 15: train_loss=0.00688, val_loss=1.59429, val_acc=0.59494
Epoch 16: train_loss=0.01198, val_loss=1.59048, val_acc=0.59494
Epoch 16: train_loss=0.00681, val_loss=1.58719, val_acc=0.62025
Epoch 16: train_loss=0.00775, val_loss=1.58413, val_acc=0.62025
Epoch 16: train_loss=0.00721, val_loss=1.58074, val_acc=0.62025
Epoch 16: train_loss=0.00714, val_loss=1.57708, val_acc=0.62025
Epoch 17: train_loss=0.00826, val_loss=1.57362, val_acc=0.60759
Epoch 17: train_loss=0.00498, val_loss=1.57012, val_acc=0.60759
Epoch 17: train_loss=0.00679, val_loss=1.56678, val_acc=0.60759
Epoch 17: train_loss=0.00826, val_loss=1.56279, val_acc=0.60759
Epoch 17: train_loss=0.00631, val_loss=1.55891, val_acc=0.60759
Epoch 18: train_loss=0.00453, val_loss=1.55548, val_acc=0.62025
Epoch 18: train_loss=0.00436, val_loss=1.55245, val_acc=0.62025
Epoch 18: train_loss=0.00712, val_loss=1.54975, val_acc=0.62025
Epoch 18: train_loss=0.00686, val_loss=1.54691, val_acc=0.63291
Epoch 18: train_loss=0.00594, val_loss=1.54409, val_acc=0.63291
Epoch 19: train_loss=0.00621, val_loss=1.54161, val_acc=0.63291
Epoch 19: train_loss=0.00437, val_loss=1.53942, val_acc=0.63291
Epoch 19: train_loss=0.00620, val_loss=1.53736, val_acc=0.62025
Epoch 19: train_loss=0.00462, val_loss=1.53559, val_acc=0.62025
Epoch 19: train_loss=0.00388, val_loss=1.53405, val_acc=0.62025
Epoch 20: train_loss=0.00532, val_loss=1.53282, val_acc=0.62025
Epoch 20: train_loss=0.00507, val_loss=1.53175, val_acc=0.62025
Epoch 20: train_loss=0.00421, val_loss=1.53096, val_acc=0.62025
Epoch 20: train_loss=0.00327, val_loss=1.52990, val_acc=0.62025
Epoch 20: train_loss=0.00459, val_loss=1.52908, val_acc=0.62025
Epoch 21: train_loss=0.00345, val_loss=1.52825, val_acc=0.62025
Epoch 21: train_loss=0.00314, val_loss=1.52781, val_acc=0.62025
Epoch 21: train_loss=0.00419, val_loss=1.52751, val_acc=0.62025
Epoch 21: train_loss=0.00373, val_loss=1.52713, val_acc=0.62025
Epoch 21: train_loss=0.00603, val_loss=1.52681, val_acc=0.62025
Epoch 22: train_loss=0.00411, val_loss=1.52661, val_acc=0.62025
Epoch 22: train_loss=0.00394, val_loss=1.52634, val_acc=0.62025
Epoch 22: train_loss=0.00356, val_loss=1.52618, val_acc=0.62025
Epoch 22: train_loss=0.00338, val_loss=1.52615, val_acc=0.62025
Epoch 22: train_loss=0.00401, val_loss=1.52619, val_acc=0.62025
Epoch 23: train_loss=0.00331, val_loss=1.52621, val_acc=0.62025
Epoch 23: train_loss=0.00347, val_loss=1.52621, val_acc=0.62025
Epoch 23: train_loss=0.00329, val_loss=1.52636, val_acc=0.62025
Epoch 23: train_loss=0.00403, val_loss=1.52648, val_acc=0.62025
Epoch 23: train_loss=0.00361, val_loss=1.52678, val_acc=0.62025
Epoch 24: train_loss=0.00275, val_loss=1.52711, val_acc=0.62025
Epoch 24: train_loss=0.00244, val_loss=1.52745, val_acc=0.62025
Epoch 24: train_loss=0.00469, val_loss=1.52770, val_acc=0.62025
Epoch 24: train_loss=0.00366, val_loss=1.52809, val_acc=0.62025
Epoch 24: train_loss=0.00332, val_loss=1.52840, val_acc=0.62025
Epoch 25: train_loss=0.00315, val_loss=1.52878, val_acc=0.62025
Epoch 25: train_loss=0.00268, val_loss=1.52901, val_acc=0.62025
Epoch 25: train_loss=0.00299, val_loss=1.52907, val_acc=0.62025
Epoch 25: train_loss=0.00350, val_loss=1.52920, val_acc=0.62025
Epoch 25: train_loss=0.00394, val_loss=1.52941, val_acc=0.62025
Epoch 26: train_loss=0.00292, val_loss=1.52967, val_acc=0.62025
Epoch 26: train_loss=0.00285, val_loss=1.52985, val_acc=0.62025
Epoch 26: train_loss=0.00339, val_loss=1.52965, val_acc=0.62025
Epoch 26: train_loss=0.00306, val_loss=1.52976, val_acc=0.62025
Epoch 26: train_loss=0.00336, val_loss=1.52985, val_acc=0.62025
Epoch 27: train_loss=0.00351, val_loss=1.53005, val_acc=0.62025
Epoch 27: train_loss=0.00303, val_loss=1.52993, val_acc=0.63291
Epoch 27: train_loss=0.00271, val_loss=1.53001, val_acc=0.63291
Epoch 27: train_loss=0.00261, val_loss=1.52996, val_acc=0.63291
Epoch 27: train_loss=0.00320, val_loss=1.52997, val_acc=0.63291
Epoch 28: train_loss=0.00308, val_loss=1.52982, val_acc=0.63291
Epoch 28: train_loss=0.00237, val_loss=1.52963, val_acc=0.63291
Epoch 28: train_loss=0.00284, val_loss=1.52929, val_acc=0.64557
Epoch 28: train_loss=0.00305, val_loss=1.52933, val_acc=0.64557
Epoch 28: train_loss=0.00329, val_loss=1.52934, val_acc=0.64557
Epoch 29: train_loss=0.00271, val_loss=1.52943, val_acc=0.63291
Epoch 29: train_loss=0.00305, val_loss=1.52947, val_acc=0.63291
Epoch 29: train_loss=0.00218, val_loss=1.52960, val_acc=0.63291
Epoch 29: train_loss=0.00321, val_loss=1.52955, val_acc=0.63291
Epoch 29: train_loss=0.00302, val_loss=1.52951, val_acc=0.63291
Epoch 30: train_loss=0.00272, val_loss=1.52939, val_acc=0.63291
Epoch 30: train_loss=0.00233, val_loss=1.52915, val_acc=0.63291
Epoch 30: train_loss=0.00342, val_loss=1.52912, val_acc=0.63291
Epoch 30: train_loss=0.00247, val_loss=1.52909, val_acc=0.64557
Epoch 30: train_loss=0.00282, val_loss=1.52909, val_acc=0.64557
Epoch 31: train_loss=0.00259, val_loss=1.52901, val_acc=0.64557
Epoch 31: train_loss=0.00285, val_loss=1.52917, val_acc=0.64557
Epoch 31: train_loss=0.00234, val_loss=1.52936, val_acc=0.64557
Epoch 31: train_loss=0.00277, val_loss=1.52951, val_acc=0.64557
Epoch 31: train_loss=0.00288, val_loss=1.52945, val_acc=0.64557
Epoch 32: train_loss=0.00345, val_loss=1.52956, val_acc=0.64557
Epoch 32: train_loss=0.00246, val_loss=1.52966, val_acc=0.64557
Epoch 32: train_loss=0.00191, val_loss=1.52982, val_acc=0.64557
Epoch 32: train_loss=0.00254, val_loss=1.52984, val_acc=0.64557
Epoch 32: train_loss=0.00274, val_loss=1.52971, val_acc=0.64557
Epoch 33: train_loss=0.00316, val_loss=1.52954, val_acc=0.64557
Epoch 33: train_loss=0.00189, val_loss=1.52931, val_acc=0.64557
Epoch 33: train_loss=0.00244, val_loss=1.52903, val_acc=0.64557
Epoch 33: train_loss=0.00219, val_loss=1.52882, val_acc=0.64557
Epoch 33: train_loss=0.00312, val_loss=1.52880, val_acc=0.64557
Epoch 34: train_loss=0.00191, val_loss=1.52871, val_acc=0.64557
Epoch 34: train_loss=0.00321, val_loss=1.52860, val_acc=0.64557
Epoch 34: train_loss=0.00255, val_loss=1.52878, val_acc=0.64557
Epoch 34: train_loss=0.00243, val_loss=1.52892, val_acc=0.64557
Epoch 34: train_loss=0.00229, val_loss=1.52890, val_acc=0.64557
Epoch 35: train_loss=0.00228, val_loss=1.52890, val_acc=0.64557
Epoch 35: train_loss=0.00193, val_loss=1.52892, val_acc=0.64557
Epoch 35: train_loss=0.00305, val_loss=1.52902, val_acc=0.64557
Epoch 35: train_loss=0.00206, val_loss=1.52900, val_acc=0.64557
Epoch 35: train_loss=0.00286, val_loss=1.52900, val_acc=0.64557
Epoch 36: train_loss=0.00258, val_loss=1.52909, val_acc=0.64557
Epoch 36: train_loss=0.00189, val_loss=1.52910, val_acc=0.64557
Epoch 36: train_loss=0.00263, val_loss=1.52898, val_acc=0.64557
Epoch 36: train_loss=0.00250, val_loss=1.52890, val_acc=0.64557
Epoch 36: train_loss=0.00221, val_loss=1.52894, val_acc=0.64557
Epoch 37: train_loss=0.00301, val_loss=1.52905, val_acc=0.64557
Epoch 37: train_loss=0.00275, val_loss=1.52895, val_acc=0.64557
Epoch 37: train_loss=0.00201, val_loss=1.52895, val_acc=0.64557
Epoch 37: train_loss=0.00204, val_loss=1.52907, val_acc=0.64557
Epoch 37: train_loss=0.00170, val_loss=1.52910, val_acc=0.64557
Epoch 38: train_loss=0.00199, val_loss=1.52913, val_acc=0.64557
Epoch 38: train_loss=0.00262, val_loss=1.52902, val_acc=0.64557
Epoch 38: train_loss=0.00206, val_loss=1.52912, val_acc=0.64557
Epoch 38: train_loss=0.00212, val_loss=1.52926, val_acc=0.64557
Epoch 38: train_loss=0.00258, val_loss=1.52926, val_acc=0.64557
Epoch 39: train_loss=0.00197, val_loss=1.52928, val_acc=0.64557
Epoch 39: train_loss=0.00179, val_loss=1.52918, val_acc=0.64557
Epoch 39: train_loss=0.00233, val_loss=1.52903, val_acc=0.64557
Epoch 39: train_loss=0.00286, val_loss=1.52914, val_acc=0.64557
Epoch 39: train_loss=0.00213, val_loss=1.52915, val_acc=0.64557
Epoch 40: train_loss=0.00171, val_loss=1.52908, val_acc=0.64557
Epoch 40: train_loss=0.00222, val_loss=1.52920, val_acc=0.64557
Epoch 40: train_loss=0.00252, val_loss=1.52919, val_acc=0.64557
Epoch 40: train_loss=0.00190, val_loss=1.52933, val_acc=0.64557
Epoch 40: train_loss=0.00255, val_loss=1.52934, val_acc=0.64557
Epoch 41: train_loss=0.00207, val_loss=1.52933, val_acc=0.64557
Epoch 41: train_loss=0.00247, val_loss=1.52929, val_acc=0.64557
Epoch 41: train_loss=0.00176, val_loss=1.52912, val_acc=0.64557
Epoch 41: train_loss=0.00234, val_loss=1.52900, val_acc=0.64557
Epoch 41: train_loss=0.00197, val_loss=1.52911, val_acc=0.64557
Epoch 42: train_loss=0.00204, val_loss=1.52921, val_acc=0.64557
Epoch 42: train_loss=0.00184, val_loss=1.52937, val_acc=0.64557
Epoch 42: train_loss=0.00205, val_loss=1.52952, val_acc=0.64557
Epoch 42: train_loss=0.00219, val_loss=1.52956, val_acc=0.64557
Epoch 42: train_loss=0.00232, val_loss=1.52967, val_acc=0.64557
Epoch 43: train_loss=0.00214, val_loss=1.52974, val_acc=0.64557
Epoch 43: train_loss=0.00216, val_loss=1.52989, val_acc=0.64557
Epoch 43: train_loss=0.00250, val_loss=1.53010, val_acc=0.64557
Epoch 43: train_loss=0.00154, val_loss=1.53033, val_acc=0.64557
Epoch 43: train_loss=0.00186, val_loss=1.53036, val_acc=0.64557
Epoch 44: train_loss=0.00157, val_loss=1.53041, val_acc=0.64557
Epoch 44: train_loss=0.00225, val_loss=1.53053, val_acc=0.64557
Epoch 44: train_loss=0.00172, val_loss=1.53081, val_acc=0.64557
Epoch 44: train_loss=0.00236, val_loss=1.53090, val_acc=0.64557
Epoch 44: train_loss=0.00213, val_loss=1.53085, val_acc=0.64557
Epoch 45: train_loss=0.00154, val_loss=1.53082, val_acc=0.64557
Epoch 45: train_loss=0.00214, val_loss=1.53052, val_acc=0.64557
Epoch 45: train_loss=0.00206, val_loss=1.53036, val_acc=0.64557
Epoch 45: train_loss=0.00207, val_loss=1.53031, val_acc=0.64557
Epoch 45: train_loss=0.00200, val_loss=1.53034, val_acc=0.64557
Epoch 46: train_loss=0.00150, val_loss=1.53028, val_acc=0.64557
Epoch 46: train_loss=0.00207, val_loss=1.53019, val_acc=0.64557
Epoch 46: train_loss=0.00241, val_loss=1.53011, val_acc=0.64557
Epoch 46: train_loss=0.00198, val_loss=1.53021, val_acc=0.64557
Epoch 46: train_loss=0.00163, val_loss=1.53028, val_acc=0.64557
Epoch 47: train_loss=0.00257, val_loss=1.53035, val_acc=0.64557
Epoch 47: train_loss=0.00171, val_loss=1.53023, val_acc=0.64557
Epoch 47: train_loss=0.00176, val_loss=1.53012, val_acc=0.64557
Epoch 47: train_loss=0.00151, val_loss=1.53010, val_acc=0.64557
Epoch 47: train_loss=0.00191, val_loss=1.53023, val_acc=0.64557
Epoch 48: train_loss=0.00134, val_loss=1.53037, val_acc=0.64557
Epoch 48: train_loss=0.00230, val_loss=1.53037, val_acc=0.64557
Epoch 48: train_loss=0.00165, val_loss=1.53054, val_acc=0.64557
Epoch 48: train_loss=0.00187, val_loss=1.53057, val_acc=0.64557
Epoch 48: train_loss=0.00215, val_loss=1.53071, val_acc=0.64557
Epoch 49: train_loss=0.00168, val_loss=1.53088, val_acc=0.64557
Epoch 49: train_loss=0.00175, val_loss=1.53108, val_acc=0.64557
Epoch 49: train_loss=0.00161, val_loss=1.53149, val_acc=0.64557
Epoch 49: train_loss=0.00203, val_loss=1.53173, val_acc=0.64557
Epoch 49: train_loss=0.00208, val_loss=1.53174, val_acc=0.64557
Epoch 50: train_loss=0.00196, val_loss=1.53170, val_acc=0.64557
Epoch 50: train_loss=0.00158, val_loss=1.53171, val_acc=0.64557
Epoch 50: train_loss=0.00167, val_loss=1.53173, val_acc=0.64557
Epoch 50: train_loss=0.00191, val_loss=1.53183, val_acc=0.64557
Epoch 50: train_loss=0.00183, val_loss=1.53182, val_acc=0.64557

Model:
import torch.nn as nn
import torch

class MaqamCNN1(nn.Module):
    def __init__(self):
        super(MaqamCNN1, self).__init__()
        
        self.conv1 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(3,3), padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=(1,1))
        self.dropout1 = nn.Dropout(p=0.1)
        
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3), padding=0)
        self.bn2 = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(kernel_size=(1,1))
        self.dropout2 = nn.Dropout(p=0.2)

        self.fc1 = nn.Linear(90016, 512)
        self.dropout3 = nn.Dropout(p=0.2)

        self.fc2 = nn.Linear(512, 265)
        self.dropout4 = nn.Dropout(p=0.2)

        self.fc3 = nn.Linear(265, 100)
        self.dropout5 = nn.Dropout(p=0.2)

    def forward(self, x):
        # print("X.shape1 = ", x.shape)
        # x = x.unsqueeze(-1)
        x = x.unsqueeze(-1)
        # print("1x.shape:", x.shape)
        # print("mfcc.shape:", mfcc.shape)
        # x = torch.squeeze(x, 3)
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.pool1(x)
        x = self.dropout1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.pool2(x)
        x = self.dropout2(x)
        # print("2x.shape:", x.shape)
        x = x.view(x.shape[0], -1)
        # print("3x.shape:", x.shape)
        x = self.fc1(x)
        # print("4x.shape:", x.shape)
        x = self.dropout3(x)

        x = self.fc2(x)
        # print("5x.shape:", x.shape)
        x = self.dropout4(x)
        x = self.fc3(x)
        # print("6x.shape:", x.shape)
        x = self.dropout5(x)
        # print("7x.shape:", x.shape)
        return x

test results:
Loading data from cache file: maqam_dataset_cache2.pkl
Test accuracy: 0.27778